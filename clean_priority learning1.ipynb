{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experimental Validation: On Emergence of Clean-Priority Learning in Early Stopped Neural Networks\n",
        "\n",
        "This notebook aims to experimentally validate the key findings presented in the paper \"On Emergence of Clean-Priority Learning in Early Stopped Neural Networks\" (Liu, Abedsoltan, Belkin, 2023, arXiv:2306.02533).\n",
        "\n",
        "**Key Phenomena to Investigate:**\n",
        "1.  **Clean-Priority Learning:** Neural networks tend to learn patterns from clean data first before fitting to noisy labels.\n",
        "2.  **U-Shaped Test Error:** The test error on a clean dataset initially decreases, reaches a minimum (early stopping point), and then increases as the network starts overfitting to noisy training labels.\n",
        "3.  **Subset Dynamics:** Training error on the clean subset of training data decreases, while error on the noisy subset initially increases and then decreases.\n",
        "4.  **Gradient Dominance:** The dominance of gradients from clean samples over noisy samples diminishes over training, leading to the termination of clean-priority learning.\n",
        "\n",
        "We will use PyTorch for model implementation and training, and Matplotlib for visualizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import copy\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Data Handling and Noise Injection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NoisyDataset(Dataset):\n",
        "    \"\"\"Wrapper to add label noise to a dataset.\"\"\"\n",
        "    def __init__(self, dataset, noise_level=0.0, num_classes=10, random_seed=42):\n",
        "        self.dataset = dataset\n",
        "        self.noise_level = noise_level\n",
        "        self.num_classes = num_classes\n",
        "        self.original_labels = []\n",
        "        self.noisy_labels = []\n",
        "        self.is_noisy = [] # To track which samples had their labels flipped\n",
        "\n",
        "        if not (0.0 <= noise_level <= 1.0):\n",
        "            raise ValueError(\"Noise level must be between 0.0 and 1.0\")\n",
        "\n",
        "        np.random.seed(random_seed) # Ensure noise is consistent\n",
        "        \n",
        "        num_samples = len(self.dataset)\n",
        "        num_noisy_samples = int(noise_level * num_samples)\n",
        "        noisy_indices = np.random.choice(num_samples, size=num_noisy_samples, replace=False)\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            image, original_label = self.dataset[i]\n",
        "            self.original_labels.append(original_label)\n",
        "            \n",
        "            if i in noisy_indices:\n",
        "                self.is_noisy.append(True)\n",
        "                # Uniformly choose a different label\n",
        "                possible_new_labels = [l for l in range(num_classes) if l != original_label]\n",
        "                if not possible_new_labels: # Handle binary case where only one other label exists\n",
        "                     noisy_label = (original_label + 1) % num_classes\n",
        "                else:\n",
        "                    noisy_label = np.random.choice(possible_new_labels)\n",
        "                self.noisy_labels.append(noisy_label)\n",
        "            else:\n",
        "                self.is_noisy.append(False)\n",
        "                self.noisy_labels.append(original_label)\n",
        "                \n",
        "        self.noisy_labels = torch.tensor(self.noisy_labels, dtype=torch.long)\n",
        "        self.original_labels = torch.tensor(self.original_labels, dtype=torch.long)\n",
        "        self.is_noisy = torch.tensor(self.is_noisy, dtype=torch.bool)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, _ = self.dataset[index] # Original dataset might return original label\n",
        "        noisy_label = self.noisy_labels[index]\n",
        "        original_label = self.original_labels[index]\n",
        "        is_noisy_sample = self.is_noisy[index]\n",
        "        return image, noisy_label, original_label, is_noisy_sample, index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "def get_mnist_datasets(binary_classes=None, data_dir='./data'):\n",
        "    \"\"\"Load MNIST dataset, optionally filtering for binary classification.\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    \n",
        "    train_dataset_full = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
        "    test_dataset_full = torchvision.datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
        "\n",
        "    if binary_classes:\n",
        "        assert len(binary_classes) == 2, \"binary_classes must contain two class labels\"\n",
        "        c1, c2 = binary_classes\n",
        "        \n",
        "        # Filter train set\n",
        "        train_indices_c1 = [i for i, (_, label) in enumerate(train_dataset_full) if label == c1]\n",
        "        train_indices_c2 = [i for i, (_, label) in enumerate(train_dataset_full) if label == c2]\n",
        "        train_dataset_filtered = Subset(train_dataset_full, train_indices_c1 + train_indices_c2)\n",
        "        # Remap labels to 0 and 1\n",
        "        train_targets_filtered = torch.cat([torch.zeros(len(train_indices_c1), dtype=torch.long), \n",
        "                                          torch.ones(len(train_indices_c2), dtype=torch.long)])\n",
        "        \n",
        "        # Filter test set\n",
        "        test_indices_c1 = [i for i, (_, label) in enumerate(test_dataset_full) if label == c1]\n",
        "        test_indices_c2 = [i for i, (_, label) in enumerate(test_dataset_full) if label == c2]\n",
        "        test_dataset_filtered = Subset(test_dataset_full, test_indices_c1 + test_indices_c2)\n",
        "        test_targets_filtered = torch.cat([torch.zeros(len(test_indices_c1), dtype=torch.long), \n",
        "                                         torch.ones(len(test_indices_c2), dtype=torch.long)])\n",
        "        \n",
        "        # Create new datasets with remapped labels\n",
        "        class RemappedDataset(Dataset):\n",
        "            def __init__(self, subset_dataset, new_targets):\n",
        "                self.subset_dataset = subset_dataset\n",
        "                self.new_targets = new_targets\n",
        "            def __getitem__(self, index):\n",
        "                image, _ = self.subset_dataset[index]\n",
        "                return image, self.new_targets[index]\n",
        "            def __len__(self):\n",
        "                return len(self.subset_dataset)\n",
        "\n",
        "        train_dataset = RemappedDataset(train_dataset_filtered, train_targets_filtered)\n",
        "        test_dataset = RemappedDataset(test_dataset_filtered, test_targets_filtered)\n",
        "        num_classes = 2 # This indicates the number of distinct classes in the dataset\n",
        "    else:\n",
        "        train_dataset = train_dataset_full\n",
        "        test_dataset = test_dataset_full\n",
        "        num_classes = 10\n",
        "        \n",
        "    return train_dataset, test_dataset, num_classes\n",
        "\n",
        "def get_cifar10_datasets(data_dir='./data'):\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform_train)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform_test)\n",
        "    return train_dataset, test_dataset, 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FCN(nn.Module):\n",
        "    \"\"\"Fully Connected Network for MNIST (binary or multi-class).\"\"\"\n",
        "    def __init__(self, input_dim=28*28, hidden_dims=[512, 512], num_classes=10):\n",
        "        super(FCN, self).__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            prev_dim = hidden_dim\n",
        "        layers.append(nn.Linear(prev_dim, num_classes))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "        return self.network(x)\n",
        "\n",
        "class CNN_MNIST(nn.Module):\n",
        "    \"\"\"CNN for MNIST as described in paper's appendix (Fig 7, 10).\"\"\"\n",
        "    # Based on: two conv layers (32, 64 channels, 3x3 kernel, stride 1), \n",
        "    # one max pooling, two FC layers (64, 10).\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNN_MNIST, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # MNIST images are 28x28. After one 2x2 pooling, they become 14x14.\n",
        "        self.fc1 = nn.Linear(64 * 14 * 14, 64)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# BasicBlock and ResNet for CIFAR-10 (ResNet-9 like structure)\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet9_CIFAR(nn.Module):\n",
        "    # A common ResNet-9 structure for CIFAR-10\n",
        "    def __init__(self, num_blocks=[1,1,1,1], num_classes=10):\n",
        "        super(ResNet9_CIFAR, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(BasicBlock, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(BasicBlock, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(BasicBlock, 256, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(256*BasicBlock.expansion, num_classes)\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride_val):\n",
        "        strides = [stride_val] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for current_stride in strides: \n",
        "            layers.append(block(self.in_planes, planes, current_stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x))) \n",
        "        x = self.layer1(x) \n",
        "        x = self.layer2(x) \n",
        "        x = self.layer3(x) \n",
        "        x = self.adaptive_pool(x) \n",
        "        x = x.view(x.size(0), -1) \n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "class ResNet9_CIFAR_Simple(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=10):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.prep_layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.layer1_conv = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
        "        self.layer1_pool = nn.MaxPool2d(2, 2) # 32x32 -> 16x16\n",
        "        self.layer1_bn = nn.BatchNorm2d(128)\n",
        "        self.layer1_relu = nn.ReLU()\n",
        "        \n",
        "        self.res1 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.layer2_conv = nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False)\n",
        "        self.layer2_pool = nn.MaxPool2d(2, 2) # 16x16 -> 8x8\n",
        "        self.layer2_bn = nn.BatchNorm2d(256)\n",
        "        self.layer2_relu = nn.ReLU()\n",
        "        \n",
        "        self.res2 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.pool3 = nn.MaxPool2d(2,2) # 8x8 -> 4x4 \n",
        "        self.classifier = nn.Linear(256 * 4 * 4, num_classes) \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.prep_layer(x)\n",
        "        \n",
        "        x = self.layer1_conv(x)\n",
        "        x = self.layer1_pool(x)\n",
        "        x = self.layer1_relu(self.layer1_bn(x))\n",
        "        \n",
        "        x = self.res1(x) + x # Residual connection 1\n",
        "        \n",
        "        x = self.layer2_conv(x)\n",
        "        x = self.layer2_pool(x)\n",
        "        x = self.layer2_relu(self.layer2_bn(x))\n",
        "        \n",
        "        x = self.res2(x) + x # Residual connection 2\n",
        "        \n",
        "        x = self.pool3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Training and Evaluation Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_loss_and_accuracy(model, dataloader, criterion, device, is_noisy_dataset=False, store_sample_losses=False, num_classes_in_dataset=10):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_preds = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    clean_loss_sum = 0; noisy_loss_sum = 0\n",
        "    clean_correct = 0; noisy_correct = 0\n",
        "    num_clean_samples = 0; num_noisy_samples = 0\n",
        "    clean_residual_sum = 0; noisy_residual_sum = 0\n",
        "\n",
        "    all_sample_indices = []\n",
        "    all_sample_is_noisy_flags = []\n",
        "    all_sample_original_labels = []\n",
        "    all_sample_losses_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data in dataloader: \n",
        "            if is_noisy_dataset:\n",
        "                images, noisy_labels, original_labels, is_noisy_flags, sample_indices = batch_data\n",
        "                targets_for_loss = noisy_labels \n",
        "                targets_for_accuracy = noisy_labels\n",
        "            else: \n",
        "                images, original_labels = batch_data\n",
        "                targets_for_loss = original_labels\n",
        "                targets_for_accuracy = original_labels\n",
        "                is_noisy_flags = torch.zeros_like(original_labels, dtype=torch.bool)\n",
        "                current_batch_size = images.size(0)\n",
        "                sample_indices = torch.arange(total_samples, total_samples + current_batch_size, device=device if images.is_cuda else 'cpu')\n",
        "\n",
        "            images = images.to(device)\n",
        "            targets_for_loss = targets_for_loss.to(device)\n",
        "            targets_for_accuracy = targets_for_accuracy.to(device) # For direct comparison with predicted class index\n",
        "            original_labels = original_labels.to(device)\n",
        "            is_noisy_flags = is_noisy_flags.to(device)\n",
        "            sample_indices = sample_indices.to(device)\n",
        "\n",
        "            outputs = model(images) # Model output, e.g., logits\n",
        "\n",
        "            # Prepare targets for loss function based on criterion type\n",
        "            current_targets_for_loss_fn = targets_for_loss\n",
        "            is_binary_bce_case = isinstance(criterion, nn.BCEWithLogitsLoss) and outputs.shape[-1] == 1\n",
        "            if is_binary_bce_case:\n",
        "                current_targets_for_loss_fn = targets_for_loss.float().unsqueeze(1)\n",
        "            \n",
        "            # Calculate loss per sample\n",
        "            loss_per_sample_val = torch.zeros(images.size(0), device=device)\n",
        "            if hasattr(criterion, 'reduction'):\n",
        "                original_reduction = criterion.reduction\n",
        "                criterion.reduction = 'none' \n",
        "                loss_per_sample_val = criterion(outputs, current_targets_for_loss_fn)\n",
        "                criterion.reduction = original_reduction \n",
        "            else:\n",
        "                 loss_per_sample_val = criterion(outputs, current_targets_for_loss_fn) \n",
        "                 if loss_per_sample_val.ndim == 0:\n",
        "                      loss_per_sample_val = loss_per_sample_val.repeat(images.size(0))\n",
        "            \n",
        "            loss = loss_per_sample_val.mean()\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Determine predicted classes for accuracy calculation\n",
        "            if is_binary_bce_case:\n",
        "                predicted = (outputs.data > 0.0).long().squeeze(1) \n",
        "            else: \n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "            \n",
        "            total_samples += images.size(0)\n",
        "            correct_preds += (predicted == targets_for_accuracy).sum().item()\n",
        "\n",
        "            if store_sample_losses:\n",
        "                all_sample_indices.extend(sample_indices.cpu().tolist())\n",
        "                all_sample_is_noisy_flags.extend(is_noisy_flags.cpu().tolist())\n",
        "                all_sample_original_labels.extend(original_labels.cpu().tolist())\n",
        "                all_sample_losses_values.extend(loss_per_sample_val.cpu().tolist())\n",
        "\n",
        "            if is_noisy_dataset:\n",
        "                clean_mask = ~is_noisy_flags\n",
        "                noisy_mask = is_noisy_flags\n",
        "\n",
        "                if clean_mask.any():\n",
        "                    clean_loss_sum += loss_per_sample_val[clean_mask].sum().item()\n",
        "                    clean_correct += (predicted[clean_mask] == targets_for_accuracy[clean_mask]).sum().item()\n",
        "                    num_clean_samples += clean_mask.sum().item()\n",
        "                    if is_binary_bce_case:\n",
        "                         clean_residual_sum += torch.abs(torch.sigmoid(outputs[clean_mask].squeeze(1)) - targets_for_accuracy[clean_mask].float()).sum().item()\n",
        "                    else: # Multi-class residual (1 - P(true_class))\n",
        "                         probs = torch.softmax(outputs[clean_mask], dim=1)\n",
        "                         true_class_probs = probs.gather(1, targets_for_accuracy[clean_mask].unsqueeze(1)).squeeze(1)\n",
        "                         clean_residual_sum += (1.0 - true_class_probs).sum().item()\n",
        "                \n",
        "                if noisy_mask.any():\n",
        "                    noisy_loss_sum += loss_per_sample_val[noisy_mask].sum().item()\n",
        "                    noisy_correct += (predicted[noisy_mask] == targets_for_accuracy[noisy_mask]).sum().item()\n",
        "                    num_noisy_samples += noisy_mask.sum().item()\n",
        "                    if is_binary_bce_case:\n",
        "                        noisy_residual_sum += torch.abs(torch.sigmoid(outputs[noisy_mask].squeeze(1)) - targets_for_accuracy[noisy_mask].float()).sum().item()\n",
        "                    else: # Multi-class residual\n",
        "                        probs = torch.softmax(outputs[noisy_mask], dim=1)\n",
        "                        true_class_probs = probs.gather(1, targets_for_accuracy[noisy_mask].unsqueeze(1)).squeeze(1)\n",
        "                        noisy_residual_sum += (1.0 - true_class_probs).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
        "    accuracy = correct_preds / total_samples if total_samples > 0 else 0\n",
        "    \n",
        "    results = {\n",
        "        'loss': avg_loss,\n",
        "        'accuracy': accuracy\n",
        "    }\n",
        "    \n",
        "    if is_noisy_dataset:\n",
        "        results['clean_loss'] = clean_loss_sum / num_clean_samples if num_clean_samples > 0 else 0\n",
        "        results['noisy_loss'] = noisy_loss_sum / num_noisy_samples if num_noisy_samples > 0 else 0\n",
        "        results['clean_accuracy'] = clean_correct / num_clean_samples if num_clean_samples > 0 else 0\n",
        "        results['noisy_accuracy'] = noisy_correct / num_noisy_samples if num_noisy_samples > 0 else 0\n",
        "        results['avg_clean_residual'] = clean_residual_sum / num_clean_samples if num_clean_samples > 0 else 0\n",
        "        results['avg_noisy_residual'] = noisy_residual_sum / num_noisy_samples if num_noisy_samples > 0 else 0\n",
        "        results['num_clean_samples'] = num_clean_samples\n",
        "        results['num_noisy_samples'] = num_noisy_samples\n",
        "    \n",
        "    if store_sample_losses:\n",
        "        clean_sample_losses_sum = 0\n",
        "        noisy_sample_losses_sum = 0\n",
        "        num_clean_for_grad_proxy = 0\n",
        "        num_noisy_for_grad_proxy = 0\n",
        "\n",
        "        for i in range(len(all_sample_indices)):\n",
        "            if all_sample_is_noisy_flags[i]:\n",
        "                noisy_sample_losses_sum += all_sample_losses_values[i]\n",
        "                num_noisy_for_grad_proxy += 1\n",
        "            else:\n",
        "                clean_sample_losses_sum += all_sample_losses_values[i]\n",
        "                num_clean_for_grad_proxy += 1\n",
        "        \n",
        "        results['proxy_g_clean_norm'] = clean_sample_losses_sum \n",
        "        results['proxy_g_noise_norm'] = noisy_sample_losses_sum\n",
        "        results['num_clean_for_grad_proxy'] = num_clean_for_grad_proxy\n",
        "        results['num_noisy_for_grad_proxy'] = num_noisy_for_grad_proxy\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion_train, criterion_eval, optimizer, num_epochs, device, num_classes_in_dataset, scheduler=None):\n",
        "    history = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [], 'train_accuracy': [],\n",
        "        'train_clean_loss': [], 'train_noisy_loss': [],\n",
        "        'train_clean_accuracy': [], 'train_noisy_accuracy': [],\n",
        "        'avg_train_clean_residual': [], 'avg_train_noisy_residual': [],\n",
        "        'proxy_g_clean_norm': [], 'proxy_g_noise_norm': [],\n",
        "        'test_loss': [], 'test_accuracy': []\n",
        "    }\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
        "        model.train()\n",
        "        for batch_data_train in train_loader: \n",
        "            images, noisy_labels, _, _, _ = batch_data_train\n",
        "            images, noisy_labels = images.to(device), noisy_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            \n",
        "            current_train_targets_for_loss = noisy_labels\n",
        "            if isinstance(criterion_train, nn.BCEWithLogitsLoss) and outputs.shape[-1] == 1:\n",
        "                current_train_targets_for_loss = noisy_labels.float().unsqueeze(1)\n",
        "            \n",
        "            loss = criterion_train(outputs, current_train_targets_for_loss) \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        train_eval_results = calculate_loss_and_accuracy(model, train_loader, criterion_eval, device, is_noisy_dataset=True, store_sample_losses=True, num_classes_in_dataset=num_classes_in_dataset)\n",
        "        test_eval_results = calculate_loss_and_accuracy(model, test_loader, criterion_eval, device, is_noisy_dataset=False, store_sample_losses=False, num_classes_in_dataset=num_classes_in_dataset)\n",
        "\n",
        "        history['epoch'].append(epoch)\n",
        "        history['train_loss'].append(train_eval_results['loss'])\n",
        "        history['train_accuracy'].append(train_eval_results['accuracy'])\n",
        "        history['train_clean_loss'].append(train_eval_results['clean_loss'])\n",
        "        history['train_noisy_loss'].append(train_eval_results['noisy_loss'])\n",
        "        history['train_clean_accuracy'].append(train_eval_results['clean_accuracy'])\n",
        "        history['train_noisy_accuracy'].append(train_eval_results['noisy_accuracy'])\n",
        "        history['avg_train_clean_residual'].append(train_eval_results['avg_clean_residual'])\n",
        "        history['avg_train_noisy_residual'].append(train_eval_results['avg_noisy_residual'])\n",
        "        history['proxy_g_clean_norm'].append(train_eval_results.get('proxy_g_clean_norm', np.nan))\n",
        "        history['proxy_g_noise_norm'].append(train_eval_results.get('proxy_g_noise_norm', np.nan))\n",
        "        history['test_loss'].append(test_eval_results['loss'])\n",
        "        history['test_accuracy'].append(test_eval_results['accuracy'])\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_eval_results['loss']:.4f}, Train Acc: {train_eval_results['accuracy']:.4f}, \"\n",
        "              f\"Test Loss: {test_eval_results['loss']:.4f}, Test Acc: {test_eval_results['accuracy']:.4f}\")\n",
        "        if 'clean_loss' in train_eval_results: \n",
        "            print(f\"  Clean_L: {train_eval_results['clean_loss']:.4f}, Noisy_L: {train_eval_results['noisy_loss']:.4f}, \"\n",
        "                  f\"Clean_Res: {train_eval_results['avg_clean_residual']:.4f}, Noisy_Res: {train_eval_results['avg_noisy_residual']:.4f}\")\n",
        "        if train_eval_results.get('num_clean_for_grad_proxy',0) > 0 and train_eval_results.get('num_noisy_for_grad_proxy',0) > 0:\n",
        "             g_clean_norm_val = train_eval_results.get('proxy_g_clean_norm', 0)\n",
        "             g_noise_norm_val = train_eval_results.get('proxy_g_noise_norm', 0)\n",
        "             ratio_g = g_clean_norm_val / g_noise_norm_val if g_noise_norm_val > 1e-9 else float('inf')\n",
        "             print(f\"  Proxy ||g_clean||: {g_clean_norm_val:.2f}, Proxy ||g_noise||: {g_noise_norm_val:.2f}, Ratio: {ratio_g:.2f}\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Plotting Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_dynamics(history, title_prefix=\"\"):\n",
        "    epochs = history['epoch']\n",
        "    \n",
        "    early_stopping_epoch = np.argmin(history['test_loss']) if len(history['test_loss']) > 0 else 0\n",
        "\n",
        "    plt.figure(figsize=(18, 6))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    if len(history.get('train_clean_accuracy', [])) == len(epochs):\n",
        "        plt.plot(epochs, 1 - np.array(history['train_clean_accuracy']), label='Train Error (Clean Subset)', marker='s', markevery=max(1, len(epochs)//10), linestyle=':')\n",
        "    if len(history.get('train_noisy_accuracy', [])) == len(epochs):\n",
        "        plt.plot(epochs, 1 - np.array(history['train_noisy_accuracy']), label='Train Error (Noisy Subset)', marker='o', markevery=max(1, len(epochs)//10), linestyle='--')\n",
        "    plt.axvline(early_stopping_epoch, color='k', linestyle='--', label=f'Early Stop Epoch {early_stopping_epoch}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Training Error')\n",
        "    plt.title(f'{title_prefix}Training Error on Subsets')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    if len(history.get('avg_train_clean_residual', [])) == len(epochs):\n",
        "        plt.plot(epochs, history['avg_train_clean_residual'], label='Avg. Residual (Clean Subset)', marker='s', markevery=max(1, len(epochs)//10), linestyle=':')\n",
        "    if len(history.get('avg_train_noisy_residual', [])) == len(epochs):\n",
        "        plt.plot(epochs, history['avg_train_noisy_residual'], label='Avg. Residual (Noisy Subset)', marker='o', markevery=max(1, len(epochs)//10), linestyle='--')\n",
        "    plt.axvline(early_stopping_epoch, color='k', linestyle='--', label='Early Stop')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Avg. Training Residual |f(w;x)-y|')\n",
        "    plt.title(f'{title_prefix}Avg. Training Residual')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    if len(history.get('train_accuracy', [])) == len(epochs):\n",
        "        plt.plot(epochs, 1 - np.array(history['train_accuracy']), label='Total Training Error', marker='s', markevery=max(1, len(epochs)//10))\n",
        "    if len(history.get('test_accuracy', [])) == len(epochs):\n",
        "        plt.plot(epochs, 1 - np.array(history['test_accuracy']), label='Test Error (Clean Test Set)', marker='o', markevery=max(1, len(epochs)//10))\n",
        "    plt.axvline(early_stopping_epoch, color='k', linestyle='--', label='Early Stop')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Classification Error')\n",
        "    plt.title(f'{title_prefix}Total Train vs. Test Error')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if len(history.get('proxy_g_clean_norm', [])) == len(epochs) and len(history.get('proxy_g_noise_norm', [])) == len(epochs):\n",
        "        g_noise_norm_np = np.array(history['proxy_g_noise_norm'])\n",
        "        g_clean_norm_np = np.array(history['proxy_g_clean_norm'])\n",
        "        \n",
        "        valid_indices = (g_noise_norm_np != 0) & (~np.isnan(g_noise_norm_np)) & (~np.isnan(g_clean_norm_np))\n",
        "        ratio_g = np.full_like(g_clean_norm_np, np.nan)\n",
        "        if np.any(valid_indices):\n",
        "           ratio_g[valid_indices] = g_clean_norm_np[valid_indices] / g_noise_norm_np[valid_indices]\n",
        "\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(epochs, ratio_g, label='Ratio ||g_clean|| / ||g_noise|| (Proxy)', marker='.')\n",
        "        plt.axhline(1.0, color='r', linestyle='--', label='y=1')\n",
        "        plt.axvline(early_stopping_epoch, color='k', linestyle='--', label='Early Stop')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Ratio of Gradient Norm Proxies')\n",
        "        plt.title(f'{title_prefix}Diminishing Dominance (Proxy)')\n",
        "        non_nan_ratio_g = ratio_g[~np.isnan(ratio_g)]\n",
        "        if len(non_nan_ratio_g) > 0:\n",
        "            plt.ylim(bottom=0, top=max(non_nan_ratio_g.max() * 1.1, 2.0))\n",
        "        else:\n",
        "            plt.ylim(bottom=0, top=2.0) \n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Proxy gradient norm data not available or incomplete for plotting ratio.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Experiment 1: Binary Classification on MNIST\n",
        "\n",
        "Following Section 3 & 4 of the paper, we'll use two classes from MNIST (e.g., '7' vs '9' or '0' vs '1') and a Fully Connected Network (FCN)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary classification: Using classes [0, 1], remapped to 0 and 1.\n",
            "Original train samples: 12665, Original test samples: 2115\n",
            "Noisy train samples: 12665. Noise level: 0.4\n",
            "Actual noisy samples: 5066 out of 12665\n",
            "\n",
            "Starting Experiment 1: Binary MNIST Classification...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20899107d9fc45ff9ef14ef23161e655",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for +=: 'int' and 'list'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m optimizer_binary \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(fcn_model_binary\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR_BINARY, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Experiment 1: Binary MNIST Classification...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m history_binary \u001b[38;5;241m=\u001b[39m train_model(fcn_model_binary, \n\u001b[0;32m     33\u001b[0m                              train_loader_binary, \n\u001b[0;32m     34\u001b[0m                              test_loader_binary, \n\u001b[0;32m     35\u001b[0m                              criterion_train_binary, \n\u001b[0;32m     36\u001b[0m                              criterion_eval_binary, \n\u001b[0;32m     37\u001b[0m                              optimizer_binary, \n\u001b[0;32m     38\u001b[0m                              NUM_EPOCHS_BINARY, \n\u001b[0;32m     39\u001b[0m                              DEVICE,\n\u001b[0;32m     40\u001b[0m                              num_classes_in_dataset\u001b[38;5;241m=\u001b[39mnum_classes_in_binary_dataset)\n\u001b[0;32m     42\u001b[0m plot_training_dynamics(history_binary, title_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBinary MNIST (FCN) - \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[4], line 173\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, test_loader, criterion_train, criterion_eval, optimizer, num_epochs, device, num_classes_in_dataset, scheduler)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler:\n\u001b[0;32m    171\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 173\u001b[0m train_eval_results \u001b[38;5;241m=\u001b[39m calculate_loss_and_accuracy(model, train_loader, criterion_eval, device, is_noisy_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, store_sample_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_classes_in_dataset\u001b[38;5;241m=\u001b[39mnum_classes_in_dataset)\n\u001b[0;32m    174\u001b[0m test_eval_results \u001b[38;5;241m=\u001b[39m calculate_loss_and_accuracy(model, test_loader, criterion_eval, device, is_noisy_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, store_sample_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_classes_in_dataset\u001b[38;5;241m=\u001b[39mnum_classes_in_dataset)\n\u001b[0;32m    176\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(epoch)\n",
            "Cell \u001b[1;32mIn[4], line 131\u001b[0m, in \u001b[0;36mcalculate_loss_and_accuracy\u001b[1;34m(model, dataloader, criterion, device, is_noisy_dataset, store_sample_losses, num_classes_in_dataset)\u001b[0m\n\u001b[0;32m    129\u001b[0m         num_noisy_for_grad_proxy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m         clean_sample_losses_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m all_sample_losses_values[i]\n\u001b[0;32m    132\u001b[0m         num_clean_for_grad_proxy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproxy_g_clean_norm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m clean_sample_losses_sum \n",
            "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'list'"
          ]
        }
      ],
      "source": [
        "# --- Configuration for Experiment 1 ---\n",
        "BINARY_CLASSES = [0, 1] # Example: '0' vs '1'\n",
        "NOISE_LEVEL_BINARY = 0.4 # As in Fig 5 (noise level delta = 0.4)\n",
        "NUM_EPOCHS_BINARY = 100 # Paper uses ~400 for Fig 5, adjust as needed for speed\n",
        "BATCH_SIZE_BINARY = 256\n",
        "LR_BINARY = 0.01 # Learning rate, adjust as needed\n",
        "HIDDEN_DIMS_FCN = [512, 512] # For FCN (2 hidden layers)\n",
        "\n",
        "# --- Data Loading ---\n",
        "mnist_train_orig, mnist_test_orig, num_classes_in_binary_dataset = get_mnist_datasets(binary_classes=BINARY_CLASSES)\n",
        "print(f\"Binary classification: Using classes {BINARY_CLASSES}, remapped to 0 and 1.\")\n",
        "print(f\"Original train samples: {len(mnist_train_orig)}, Original test samples: {len(mnist_test_orig)}\")\n",
        "\n",
        "# Add noise to training set\n",
        "noisy_mnist_train_binary = NoisyDataset(mnist_train_orig, noise_level=NOISE_LEVEL_BINARY, num_classes=num_classes_in_binary_dataset, random_seed=SEED)\n",
        "print(f\"Noisy train samples: {len(noisy_mnist_train_binary)}. Noise level: {NOISE_LEVEL_BINARY}\")\n",
        "print(f\"Actual noisy samples: {noisy_mnist_train_binary.is_noisy.sum().item()} out of {len(noisy_mnist_train_binary)}\")\n",
        "\n",
        "train_loader_binary = DataLoader(noisy_mnist_train_binary, batch_size=BATCH_SIZE_BINARY, shuffle=True)\n",
        "test_loader_binary = DataLoader(mnist_test_orig, batch_size=BATCH_SIZE_BINARY, shuffle=False) # Test set is clean\n",
        "\n",
        "# --- Model, Loss, Optimizer ---\n",
        "input_dim_mnist = 28*28\n",
        "# For binary classification with BCEWithLogitsLoss, model should output 1 logit\n",
        "fcn_model_binary = FCN(input_dim=input_dim_mnist, hidden_dims=HIDDEN_DIMS_FCN, num_classes=1).to(DEVICE)\n",
        "\n",
        "criterion_train_binary = nn.BCEWithLogitsLoss() \n",
        "criterion_eval_binary = nn.BCEWithLogitsLoss() \n",
        "optimizer_binary = optim.SGD(fcn_model_binary.parameters(), lr=LR_BINARY, momentum=0.9)\n",
        "\n",
        "print(\"\\nStarting Experiment 1: Binary MNIST Classification...\")\n",
        "history_binary = train_model(fcn_model_binary, \n",
        "                             train_loader_binary, \n",
        "                             test_loader_binary, \n",
        "                             criterion_train_binary, \n",
        "                             criterion_eval_binary, \n",
        "                             optimizer_binary, \n",
        "                             NUM_EPOCHS_BINARY, \n",
        "                             DEVICE,\n",
        "                             num_classes_in_dataset=num_classes_in_binary_dataset)\n",
        "\n",
        "plot_training_dynamics(history_binary, title_prefix=\"Binary MNIST (FCN) - \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Experiment 2: Multi-class Classification\n",
        "\n",
        "Following Section 5 of the paper, we'll use full MNIST (10 classes) with a CNN, or CIFAR-10 with a ResNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Multi-class MNIST with CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration for Experiment 2a (MNIST CNN) ---\n",
        "NOISE_LEVEL_MNIST_MULTI = 0.3 # As in Fig 7, MNIST (delta=0.3)\n",
        "NUM_EPOCHS_MNIST_MULTI = 50 # Paper uses ~180 for Fig 7, adjust for speed\n",
        "BATCH_SIZE_MNIST_MULTI = 128 # Paper uses 512, adjust based on memory\n",
        "LR_MNIST_MULTI = 0.01\n",
        "\n",
        "# --- Data Loading ---\n",
        "mnist_train_full_orig, mnist_test_full_orig, num_classes_mnist_multi = get_mnist_datasets()\n",
        "print(f\"Multi-class MNIST: Using all 10 classes.\")\n",
        "print(f\"Original train samples: {len(mnist_train_full_orig)}, Original test samples: {len(mnist_test_full_orig)}\")\n",
        "\n",
        "noisy_mnist_train_multi = NoisyDataset(mnist_train_full_orig, noise_level=NOISE_LEVEL_MNIST_MULTI, num_classes=num_classes_mnist_multi, random_seed=SEED)\n",
        "print(f\"Noisy train samples: {len(noisy_mnist_train_multi)}. Noise level: {NOISE_LEVEL_MNIST_MULTI}\")\n",
        "print(f\"Actual noisy samples: {noisy_mnist_train_multi.is_noisy.sum().item()} out of {len(noisy_mnist_train_multi)}\")\n",
        "\n",
        "train_loader_mnist_multi = DataLoader(noisy_mnist_train_multi, batch_size=BATCH_SIZE_MNIST_MULTI, shuffle=True)\n",
        "test_loader_mnist_multi = DataLoader(mnist_test_full_orig, batch_size=BATCH_SIZE_MNIST_MULTI, shuffle=False)\n",
        "\n",
        "# --- Model, Loss, Optimizer ---\n",
        "cnn_model_mnist = CNN_MNIST(num_classes=num_classes_mnist_multi).to(DEVICE)\n",
        "\n",
        "criterion_train_multi = nn.CrossEntropyLoss() \n",
        "criterion_eval_multi = nn.CrossEntropyLoss() \n",
        "optimizer_mnist_multi = optim.SGD(cnn_model_mnist.parameters(), lr=LR_MNIST_MULTI, momentum=0.9)\n",
        "\n",
        "print(\"\\nStarting Experiment 2a: Multi-class MNIST Classification (CNN)...\")\n",
        "history_mnist_multi = train_model(cnn_model_mnist, \n",
        "                                  train_loader_mnist_multi, \n",
        "                                  test_loader_mnist_multi, \n",
        "                                  criterion_train_multi, \n",
        "                                  criterion_eval_multi, \n",
        "                                  optimizer_mnist_multi, \n",
        "                                  NUM_EPOCHS_MNIST_MULTI, \n",
        "                                  DEVICE,\n",
        "                                  num_classes_in_dataset=num_classes_mnist_multi)\n",
        "\n",
        "plot_training_dynamics(history_mnist_multi, title_prefix=\"Multi-class MNIST (CNN) - \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Multi-class CIFAR-10 with ResNet-9 (Simplified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration for Experiment 2b (CIFAR-10 ResNet) ---\n",
        "NOISE_LEVEL_CIFAR10 = 0.4 # As in Fig 7, CIFAR-10 (delta=0.4)\n",
        "NUM_EPOCHS_CIFAR10 = 100 # Paper uses ~300 for Fig 7, adjust for speed\n",
        "BATCH_SIZE_CIFAR10 = 128 # Paper uses 512, adjust based on memory\n",
        "LR_CIFAR10 = 0.01 # May need tuning, or a scheduler\n",
        "\n",
        "# --- Data Loading ---\n",
        "cifar10_train_orig, cifar10_test_orig, num_classes_cifar10 = get_cifar10_datasets()\n",
        "print(f\"Multi-class CIFAR-10: Using all 10 classes.\")\n",
        "print(f\"Original train samples: {len(cifar10_train_orig)}, Original test samples: {len(cifar10_test_orig)}\")\n",
        "\n",
        "noisy_cifar10_train = NoisyDataset(cifar10_train_orig, noise_level=NOISE_LEVEL_CIFAR10, num_classes=num_classes_cifar10, random_seed=SEED)\n",
        "print(f\"Noisy train samples: {len(noisy_cifar10_train)}. Noise level: {NOISE_LEVEL_CIFAR10}\")\n",
        "print(f\"Actual noisy samples: {noisy_cifar10_train.is_noisy.sum().item()} out of {len(noisy_cifar10_train)}\")\n",
        "\n",
        "train_loader_cifar10 = DataLoader(noisy_cifar10_train, batch_size=BATCH_SIZE_CIFAR10, shuffle=True, num_workers=2)\n",
        "test_loader_cifar10 = DataLoader(cifar10_test_orig, batch_size=BATCH_SIZE_CIFAR10, shuffle=False, num_workers=2)\n",
        "\n",
        "# --- Model, Loss, Optimizer ---\n",
        "resnet_model_cifar10 = ResNet9_CIFAR(num_blocks=[1,1,1,1], num_classes=num_classes_cifar10).to(DEVICE)\n",
        "\n",
        "criterion_train_cifar10 = nn.CrossEntropyLoss()\n",
        "criterion_eval_cifar10 = nn.CrossEntropyLoss() \n",
        "optimizer_cifar10 = optim.SGD(resnet_model_cifar10.parameters(), lr=LR_CIFAR10, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler_cifar10 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_cifar10, T_max=NUM_EPOCHS_CIFAR10)\n",
        "\n",
        "print(\"\\nStarting Experiment 2b: Multi-class CIFAR-10 Classification (ResNet-9)...\")\n",
        "history_cifar10 = train_model(resnet_model_cifar10, \n",
        "                              train_loader_cifar10, \n",
        "                              test_loader_cifar10, \n",
        "                              criterion_train_cifar10, \n",
        "                              criterion_eval_cifar10, \n",
        "                              optimizer_cifar10, \n",
        "                              NUM_EPOCHS_CIFAR10, \n",
        "                              DEVICE,\n",
        "                              num_classes_in_dataset=num_classes_cifar10,\n",
        "                              scheduler=scheduler_cifar10)\n",
        "\n",
        "plot_training_dynamics(history_cifar10, title_prefix=\"Multi-class CIFAR-10 (ResNet-9) - \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Experiment 3: Effect of Network Width (Appendix C.3)\n",
        "\n",
        "We can re-run the binary MNIST experiment with varying FCN hidden layer widths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WIDTHS_TO_TEST = [32, 128, 512, 2048] # From Fig 12 in paper\n",
        "NUM_EPOCHS_WIDTH_TEST = 50 # Reduced epochs for faster testing\n",
        "NOISE_LEVEL_WIDTH_TEST = 0.4 # As in Fig 12\n",
        "\n",
        "histories_by_width = {}\n",
        "\n",
        "print(\"\\nStarting Experiment 3: Effect of Network Width...\")\n",
        "for width in WIDTHS_TO_TEST:\n",
        "    print(f\"\\n--- Testing FCN with hidden_dims = [{width}, {width}] ---\")\n",
        "    mnist_train_orig_w, mnist_test_orig_w, num_classes_w_dataset = get_mnist_datasets(binary_classes=BINARY_CLASSES)\n",
        "    noisy_mnist_train_w = NoisyDataset(mnist_train_orig_w, noise_level=NOISE_LEVEL_WIDTH_TEST, num_classes=num_classes_w_dataset, random_seed=SEED)\n",
        "    train_loader_w = DataLoader(noisy_mnist_train_w, batch_size=BATCH_SIZE_BINARY, shuffle=True)\n",
        "    test_loader_w = DataLoader(mnist_test_orig_w, batch_size=BATCH_SIZE_BINARY, shuffle=False)\n",
        "\n",
        "    fcn_model_w = FCN(input_dim=input_dim_mnist, hidden_dims=[width, width], num_classes=1).to(DEVICE) # Binary output is 1 logit\n",
        "    optimizer_w = optim.SGD(fcn_model_w.parameters(), lr=LR_BINARY, momentum=0.9)\n",
        "\n",
        "    history_w = train_model(fcn_model_w, train_loader_w, test_loader_w, \n",
        "                            criterion_train_binary, criterion_eval_binary, \n",
        "                            optimizer_w, NUM_EPOCHS_WIDTH_TEST, DEVICE, num_classes_in_dataset=num_classes_w_dataset)\n",
        "    histories_by_width[width] = history_w\n",
        "    plot_training_dynamics(history_w, title_prefix=f\"Binary MNIST (FCN Width {width}) - \")\n",
        "\n",
        "print(\"\\nExperiment 3 (Width Test) Complete. Histories stored in 'histories_by_width'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Experiment 4: Effect of Noise Level (Appendix C.4)\n",
        "\n",
        "We can re-run the multi-class MNIST experiment with varying noise levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NOISE_LEVELS_TO_TEST = [0.1, 0.2, 0.3, 0.4] # From Fig 13 in paper\n",
        "NUM_EPOCHS_NOISE_TEST = 50 # Reduced epochs\n",
        "\n",
        "histories_by_noise_level = {}\n",
        "\n",
        "print(\"\\nStarting Experiment 4: Effect of Noise Level...\")\n",
        "for noise_lvl in NOISE_LEVELS_TO_TEST:\n",
        "    print(f\"\\n--- Testing Multi-class MNIST with Noise Level = {noise_lvl} ---\")\n",
        "    mnist_train_full_orig_nl, mnist_test_full_orig_nl, num_classes_nl_dataset = get_mnist_datasets()\n",
        "    noisy_mnist_train_nl = NoisyDataset(mnist_train_full_orig_nl, noise_level=noise_lvl, num_classes=num_classes_nl_dataset, random_seed=SEED)\n",
        "    train_loader_nl = DataLoader(noisy_mnist_train_nl, batch_size=BATCH_SIZE_MNIST_MULTI, shuffle=True)\n",
        "    test_loader_nl = DataLoader(mnist_test_full_orig_nl, batch_size=BATCH_SIZE_MNIST_MULTI, shuffle=False)\n",
        "\n",
        "    cnn_model_nl = CNN_MNIST(num_classes=num_classes_nl_dataset).to(DEVICE) \n",
        "    optimizer_nl = optim.SGD(cnn_model_nl.parameters(), lr=LR_MNIST_MULTI, momentum=0.9)\n",
        "\n",
        "    history_nl = train_model(cnn_model_nl, train_loader_nl, test_loader_nl, \n",
        "                             criterion_train_multi, criterion_eval_multi, \n",
        "                             optimizer_nl, NUM_EPOCHS_NOISE_TEST, DEVICE, num_classes_in_dataset=num_classes_nl_dataset)\n",
        "    histories_by_noise_level[noise_lvl] = history_nl\n",
        "    plot_training_dynamics(history_nl, title_prefix=f\"Multi-class MNIST (CNN Noise {noise_lvl}) - \")\n",
        "\n",
        "print(\"\\nExperiment 4 (Noise Level Test) Complete. Histories stored in 'histories_by_noise_level'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Discussion and Conclusion\n",
        "\n",
        "After running the experiments, analyze the generated plots:\n",
        "1.  **U-Shaped Test Error:** Do the plots of 'Total Training Error vs. Test Error' consistently show the test error decreasing and then increasing? Mark the early stopping point.\n",
        "2.  **Clean-Priority Learning in Subsets:** \n",
        "    * Does the 'Training Error on Subsets' plot show clean subset error consistently decreasing while noisy subset error initially increases (or stays high) before eventually decreasing?\n",
        "    * Does the 'Avg. Training Residual on Subsets' plot show a similar trend, with residuals on clean data decreasing and residuals on noisy data (w.r.t. their noisy labels) initially increasing or staying high?\n",
        "3.  **Gradient Dominance Proxy:** Does the 'Diminishing Dominance of Clean Gradient (Proxy)' plot show the ratio `||g_clean|| / ||g_noise||` starting high and decreasing towards ~1.0 around the early stopping point?\n",
        "4.  **Effect of Width/Noise:** How do these dynamics change with different network widths or noise levels? Do the observations align with the paper's appendix figures (Fig 12, 13)?\n",
        "\n",
        "**Limitations:**\n",
        "* The calculation of `||g_clean|| / ||g_noise||` uses a proxy (sum of sample losses) rather than actual gradient norms with respect to model parameters, which would be more computationally intensive to track per sample for all parameters.\n",
        "* Exact reproduction of paper's figures might require fine-tuning of hyperparameters (learning rates, schedulers, specific optimizer details, exact model architectures if not fully specified, and number of epochs).\n",
        "* The ResNet-9 architectures used here are common simplified versions; the paper might use a specific variant. The `ResNet9_CIFAR` is closer to a standard ResNet block structure, while `ResNet9_CIFAR_Simple` is a more direct layer-by-layer definition often seen in tutorials.\n",
        "\n",
        "**Conclusion:**\n",
        "This notebook provides a framework for experimentally exploring the clean-priority learning phenomenon. By observing the plotted dynamics, one can gain insights into how neural networks interact with noisy labels during training and understand the importance of early stopping."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
